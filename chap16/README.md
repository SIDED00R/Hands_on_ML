## RNN을 사용한 자연어 처리

텍스트를 예측하는 모델을 만들기 위해 순차 데이터셋을 나눌 때 훈련 세트, 검증 세트, 테스트 세트가 중복되지 않도록 만드는 것이 매우 중요하다.

세트를 나눌 때 문장이 세트 사이에 걸치지 않고 완전히 분리될 수 있도록 세트 사이에 간격을 두는 것도 좋은 생각이다.

시계열 데이터의 경우 보통 시간에 따라서 데이터를 분리한다.

만약 훈련 세트와 테스트 세트에 상호 연관된 데이터가 있다면 테스트 세트에서 측정한 일반 오차가 낙관적으로 편향되어 유용하지 않다.

그리고 백 만개 이상의 글자로 이뤄진 훈련세트에 신경망을 직접 훈련시킬 수 없다.

왜냐하면 이는 백만개의 층이 있는 심층 신경망과 비슷하게 생각할 수 있고, 매우 긴 샘플 하나로 훈련하는 상황이여서 비용이 매우 높기 때문이다.

따라서 데이터셋의 window()메서드를 이용해 긴 시퀀스를 작은 많은 텍스트 윈도우로 변환한다.

이를 TBPTT라고 RNN에서 역전파 과정을 일부 제한하여 계산 효율성을 높이는 방법 중 하나이다.

TBPTT의 주요 아이디어는 RNN의 타임 스텝을 일정 구간으로 제한하고, 이 구간 내에서만 역전파를 수행하며, 구간을 넘어가는 정보를 잘라냄으로써 계산 비용을 줄이는 것이다.

이렇게 하면 긴 시퀀스에 대한 역전파를 효율적으로 처리할 수 있지만, 제한된 시간 범위 내에서만 고려되므로 장기적인 의존성에 대해서는 일부 정보가 손실될 수 있다.

### 상태가 있는 RNN

상태가 있는 RNN은 훈련 반복 사이에 은닉 상태를 유지하고, 중지된 곳에서 마지막 상태를 다음 훈련 배치의 초기 상태로 반영한다.

결과적으로 상태가 있는 RNN은 더 긴 패턴을 학습할 수 있다.

상태가 있는 RNN은 배치의 각 입력 시퀀스가 이전 배치의 시퀀스가 끝난 지점에서 시작해야하기 때문에 순차적이고 겹치지 않는 입력 시퀀스를 만들어야한다.

### 텍스트 전처리

일반적으로 공백을 사용해 단어 경계를 구분하지만, 모든 문자가 이런 식으로 공백을 사용하지 않는다.

예를 들어 중국어는 단어 사이에 공백을 사용하지 않고, 베트남어는 단어 사이에도 공백을 사용한다.

그리고 독일어와 같은 언어는 여러 단어를 공백 없이 붙여쓰고, 영어에도 상황에 따라 단어를 붙여쓰기도 띄어씌기도 한다.

따라서 텍스트를 전처리 할 때 부분 단어의 수준으로 텍스트를 토큰화 하거나 복원하는 비지도 학습을 사용하면 좋다.

이는 공백 또한 하나의 문자로 취급하여 언어 독립적이고, 모델이 이전에 본 적이 없는 단어일지라도 의미를 추측할 수 있는 장점이있다.

 - 마스킹 : 주로 시퀀스 데이터에서 특정 타임 스텝을 무시하거나 강조하기 위해 사용되는 기술이다.

### 신경망 기계 번역 (NMT)

-	인코더 디코더 모델

 	예를 들어 영어를 입력하면 한글로 번역해주는 신경망 기계 번역 모델을 생각해 보면 영어 문장을 인코더로 주입했을 때 디코더로 한국어가 출력된다.
 	
 	각 단계마다 디코더는 한국어에 있는 단어에 대한 점수를 출력해 가장 높은 확률의 단어가 출력된다.
 	
 	다만, 한국어와 영어의 문장의 길이가 다를 수 있는데, 전체 문장을 번역해야하기 때문에 길이를 맞추기 위해 그냥 잘라내버릴수 없어 문장을 비슷한 길이의 버킷으로 그룹핑한다.
 	
 	그 다음 버킷에 담긴 문장의 길이가 모두 동일해지도록 패딩을 추가하면 된다.
 	
 	그리고 출력 어휘 사전의 단어 수가 많기 때문에 모든 단어의 확률을 계산하려면 매우 느려지게된다.
 	
 	따라서 소프트맥스 함수를 계산하는 것이 연산 비용이 매우 높기 때문에 이를 피하기 위해 샘플링 소프트맥스를 사용한다.
 	
 	샘플링 소프트맥스의 주요 아이디어는 일부 무작위 클래스만을 대상으로 확률 분포를 추정하는 것이다.
  
### 양방향 RNN

타임 스텝에서 일반적인 순환층은 과거와 현재의 입력만 보고 출력을 생각한다.

시계열 데이터를 예측할 때는 이러한 방식이 적합하지만, NLP작업에는 미래의 단어에 따라 뜻하는 의미가 달라질 수 있기 때문에 이러한 방식은 맞지 않는다.

따라서 동일한 입력에 대해 두 개의 순환층을 실행하는데, 하나는 왼쪽에서 오른쪽으로 단어를 읽고 하나는 오른쪽에서 왼쪽으로 읽는다.

그리고 타임스텝마다 이 두 출력을 연결하는 양방향 순환층을 사용한다.

모델이 예측을 할 때 스텝마다 가장 가능성 잇는 단어를 출력하다보면 같은 단어라도 여러 뜻을 가지기 때문에 최적의 번역이 나오지 않는 경우가 있다.

따라서 빔 검색을 통해 앞선 실수를 고칠 수 있게 해야한다.

이는 빔 너비인 K개의 가능성 있는 문장의 리스트를 유지하고, 디코더 단계마다 문장의 단어를 하나씩 추가하여 가능성 있는 K개의 문장을 만든다.

모든 후보 시퀀스를 확장한 후에는 각 시퀀스의 점수를 다시 계산하는 과정을 반복하여 선택된 최종 후보들이 종료 조건을 만족하는지 확인하고, 만족하는 시퀀스 중에서 최적 시퀀스를 선택한다.

### 어텐션 메커니즘

각 타임스텝에서 적절한 단어에 디코더가 초점을 맞춰서 입력 단어에서 번역까지 경로가 훨씬 짧아지게되어 RNN의 단기 기억의 제한성에 훨씬 적은 영향을 받게 된다.

디코더가 어떤 단어에 더 초점을 맞출지는 가중치에 의해 결정되는데 이때 가중치는 정렬 모델이라고 부르는 작은 신경망에 의해 생성된다.

정렬 모델은 인코더-디코더 모델의 나머지 부분과 함께 훈련된다.

어텐션 메커니즘의 장점은 모델이 어떤 출력을 만들도록 이끄는 것이 무엇인지 이해하기 쉽다는 것이다.

이를 설명 가능성이라고 하는데 모델이 단어를 출력할 때 초점을 맞춘 것이 어떤것인지 쉽게 확인할 수 있다.

어텐션 메커니즘

$$
\begin{align}
\hat h_{(t)} &= \sum\limits_i \alpha_{(t, i)} y_{(i)} \\
\alpha_{(t, i)} &= \dfrac{\exp (e_{(t, i)})}{\sum_{i'}\exp (e_{(t, i)})} \\
e_{(t, i)} &=
\begin{cases}
h_{(t)}^T y_{(i)}\\
h_{(t)}^T W y_{(i)}\\
v^T \tanh (W[h_{(t)} ; y_{(i)}])\\
\end{cases}
\end{align}
$$

### 트랜스포머 구조
순환층이나 합성곱 층을 전혀 사용하지 않고 어텐션 메커니즘만 사용해 nmt문제에서 최고 수준의 성능을 크게 향상했다.
트랜스포머 구조의 추가적인 장점은 구조를 훨씬 빠르게 훈련할 수 있고 병렬화 하기 쉽다는 것이다.
ㅁㄴㅇㄹ

트랜스포머 구조의 새로운 구성요소
-	위치 인코딩
위치 인코딩은 문장 안에 있는 단어의 위치를 인코딩한 밀집벡터이다.
단순히 i번째 위치 인코딩이 문장에 있는 i번째 단어 임베딩에 더해진다.

-	멀티-헤드 어텐션
기본이 되는 스케일드 점-곱 어텐션을 이해해야한다.
인코더가 문장을 분석하여 특정 단어가 주어인지 동사인지 이해해야한다.
그러기 위해서 인코더가 딕셔너리를 만들고 디코더가 key에 해당하는 value를 찾는 과정이 필요한데 모델에는 키를 표현하기 위한 구분된 토큰이 없어 벡터 표현으로 이 개념을 갖는다.
따라서 룩업에 사용할 키는 딕셔너리의 키와 완벽하게 매칭되지 않는다.
그래서 쿼리와 딕셔너리에 있는 각 키 사이의 유사도를 계산한 다음 소프트 맥스 함수를 사용해 유사도 점수를 합해 1이 되는 가중치로 변환한다.
그 다음 도멜이 키에 해당하는 값의 가중치 합을 계산한다.
공식()

이제 멀티 헤드 어텐션은 이런 스케일 점 곱 어텐션 층의 묶음이다.
각 층은 값, 키, 쿼리의 선형변환이 선행되고, 출력은 단순히 모두 연결되어있어 마지막 선현 병환을 통과한다.
만약 멀티 헤드 어텐션을 사용하지 않으면 점-곱 어텐션으로 단어의 위치, 과거형 등과 같은 많은 특성을 모두 쿼리해야하기 때문에 여러 부분공간으로 다양하게 투영하여 더 효과적으로 단어를 표현할 수 있게 된다.


