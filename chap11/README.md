## 심층 신경망

심층 신경망의 난점

1. 그레디언트 소실 또는 폭주 문제에 직면할 수 있다.
   
2. 대규모 신경망을 위한 데이터의 수가 부족하거나 레이블을 만드는데 너무 많은 비용이 발생할 수 있다.
   
3. 훈련이 극단적으로 느려질 수 있다.
   
4. 파라미터의 수가 너무 많아 모델이 과대적합 될 위험이 매우 크다.

   특히 샘플의 노이즈가 많을 경우 이 모든 노이즈에 확습이 되어 과대적합이 될 가능성이 높다.

### 그레디언트 소실과 폭주

그레디언트 소실과 폭주는 심층 신경망의 아래쪽으로 갈수록 그레디언트가 점점 작아지거나 커지게 되는 현상이다. 

 - 그레디언트 소실
   
   그레디언트 소실은 역전파알고리즘을 사용하여 가중치를 업데이트할 때 발생한다.
   
   역전파는 출력값과 실제 값 사이의 오차를 계산하고, 이 오차를 이용하여 네트워크를 훈련시키는 과정 중에 그레디언트를 계산한다.
   
   그러나 몇 개의 레이어를 거치면서 그레디언트가 지수적으로 감소할 수 있다.
   
   즉, 네트워크의 앞쪽 레이어로 갈수록 그레디언트 값이 매우 작아져 가중치 업데이트가 거의 이루어지지 않는 상황이 발생하게 된다.
   
   이는 역전파가 더 이상 신경망을 효과적으로 학습시키지 못하게 만들어 문제를 야기한다.
   
 - 그레디언트 폭주
   
   그레디언트 폭주는 그레디언트의 값이 지수적으로 증가하여 가중치 업데이트가 매우 큰 값으로 발산하는 문제이다.
   
   이 경우, 신경망의 가중치가 너무 큰 값으로 수렴하게 되며, 이로 인해 모델이 불안정해지고 학습이 제대로 이루어지지 않는다.
   
   이 문제는 주로 순환 신경망과 같이 순차적인 데이터를 다루는 모델에서 자주 발생한다.

초기에는 심층신경망에서 그레디언트를 불안정하게 만드는 원인을 찾지 못하였다.

글로럿과 벤지오가 발표한 논문 덕분에 이제는 원인에 대한 이해가 많이 진전되었다.

의심되는 원인으로는 보통 활성화 함수로 로지스틱 시그모이드 함수를 사용하였는데, 이는 출력의 분산이 입력의 분산보다 더 커지게 된다.

따라서 신경망의 위로 갈수록 분산이 계속 커져 가장 높은 층에서는 활성함수가 0이나 1로 수렴하게 되었다.

로지스틱 함수에서 값이 0이나 1로 수렴하게 된다면 기울기가 0에 매우 가까워지기 때문에 점점 세기가 약해져 실제 역전파가 될 때 아래쪽 층에는 아무것도 도달하지 않게 된다.

그리고 로지스틱 함수의 평균이 0.5이고 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력보다 커질 가능성이 높다.

따라서 편향 이동이 발생하여 더 나빠지게 되었다. 

### 글로럿 초기화와 He초기화

- 글로럿 초기화

  예측을 할 때는 정방향으로, 역전파할때는 역방향으로 양방향 신호가 적절하게 흘러야 한다.
  
  그러기 위해서 글로럿은 출력에 대한 분산이 입력층에 대한 분산과 같아야 하고, 역방향에서 층을 통과하기 전과 후의 그레디언트 분산이 동일해야한다고 하였다.
  
  따라서 각 층의 연결 가중치를 다음과 같이 무작위로 초기화 한다.
  
  로지스틱 활성화 함수를 사용할 때dml 글로럿 초기화
  
  평균이 0이고 분산이 $\sigma^2 = \frac{1}{ fan_{avg} }$인 정규분포
  
  또는 $r = \sqrt{\frac{3}{ fan_{avg} }}$일 때 –r과 r사이의 균등분포
  
  여기서 $fan_{avg} = \frac{ fan_{in}  + fan_{out} }{2}$이고, $fan_{in}$은 해당 층의 입력 연결 개수, $fan_{out}$은 출력 연결 개수이다.
  
  글로럿 초기화를 사용하면 훈련 속도를 상당히 높일 수 있다.

- He초기화

  활성화 함수를 ReLU혹은 ReLU의 변종들에 대한 초기화 전략을 사용하는 것을 He초기화 라고 한다.
  
  ReLU는 특정 양수값에 수렴하지 않는 큰 장점이 있다.
  
  하지만 훈련하는 동안 일부 뉴런이 0이외의 값을 출력하지 않는 죽은ReLU로 알려진 문제가 있다.
  
  훈련도중 뉴런의 가중치가 바뀌어 훈련 세트에 있는 모든 샘플에 대해 입력의 가중치 합이 음수가 될 때 이러한 문제가 발생하게 된다.
  
  이러한 문제를 해결하기 위해 LeakyReLU와 같이 변종을 사용한다.
  
  $\mbox{LeakyReLU}_\alpha(z) = \max(\alpha z,z)$
  
  하이퍼파리미터 $\alpha$ 는 값이 음수일 때의 기울기로 뉴런이 절대 죽지 않게 만들어준다.

ELU함수는 새로운 활성화 함수로 기존의 모든 변종들의 성능을 앞질렀다.

$$
ELU_\alpha(z)=
\begin{cases}
\alpha(\exp(z) - 1) & \mbox{if} z < 0\\
z & \mbox{if} z \geq 0
\end{cases}
$$

ELU는 음수가 입력되면 음수값이 들어오므로 평균 출력이 0에 더 가까워 지고, 그레디언트가 0이 아니여서 죽은 뉴런을 만들지도 않는다.

그리고 알파값이 1이면 0에서 함수값이 급변하지 않아 모든 구간에서 매끄럽기 때문에 경사하강법의 속도가 높아지게 된다.

하지만 단점으로는 지수함수를 사용하기 때문에 계산하는데 시간이 걸려 살짝 느려 훈련하는 과정에서는 수렴 속도가 빨라 이 단점이 상쇄되지만 테스트시에는 느리게 된다.

### 배치정규화

각 층에서 활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가해준다.

이 연산은 단순하게 입력을 원점에 맞추고 정규화한 다음 각 층에서 두개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동시킨다.

$$
\begin{align}
&\mbox{1.} \quad \mu_B = \frac{1}{m_B} \sum\limits_{i = 1}^{m_B} x^{(i)} \\
&\mbox{2.} \quad \sigma_B^2 = \frac{1}{m_B} \sum\limits_{i = 1}^{m_B} (x^{(i)} - \mu_\beta)^2 \\ 
&\mbox{3.} \quad \hat x^{(i)} = \frac{x^{(i)} - \mu_\beta}{\sqrt{ \sigma_B^2 + \epsilon}} \\
&\mbox{4.} \quad z^{(i)} = \gamma \otimes \hat x^{(i)} + \beta
\end{align}
$$

$\mu_B$ 는 미니배치를 평가한 입력의 평균 벡터이다. 

$\sigma_B^2$ 은 미니배치에 대해 평가한 입력의 표준편차 벡터이다.

$m_B$ 은 미니배치의 샘플의 수이다.

$$평균이 0이고 정규화된 샘플i의 입력이다.

$\hat x^{(i)}$ 은 층의 출력 스케일 파라미터 벡터이다.

$\epsilon$ 은 분모가 0이 되는 것을 막기 위한 매우 작은 숫자이다.

훈련하는동안 배치 정규화는 입력을 정규화 하고 스케일을 조정해서 이동시킨다.

하지만 층마다 추가되는 계산이 시간 복잡도를 증가시켜 예측 속도가 느려지게 된다.

### 그레디언트 클리핑

그레디언트 폭주 문제를 완화하는 방법으로는 역전파 될 때 일정 임곗값을 넘지 못하게 그레디언트를 잘라내는 그레디언트 클리핑을 하면된다.

이는 미리 임계값을 설정하고, 역전파 과정에서 그레디언트를 계산할 때 임계값을 초과한다면 임계값 내로 제한하여 가중치를 업데이트 한다.

### 심층 신경망의 비용 문제

아주 큰 규모의 신경망의 경우 처음부터 새로 훈련하는 것은 비효율적이다.

따라서 비슷한 유형의 문제를 처리한 신경망의 하위층을 재사용하는 전이학습을 하는 것이 좋다.
이렇게하면 훈련 속도가 크게 빨라지고, 필요한 데이터의 수도 줄어들게 된다.
재사용에 사용하는 층은 모두 동결시켜 경사 하강법으로 가중치가 바뀌지 않도록 한다.
그 후 맨 위의 한 두개의 은닉층의 동결을 해제하고 역전파를 통해 가중치를 조정하여 성능을 평가한다.
재사용층의 동결을 해제할때는 학습률을 줄여 세밀하게 튜닝하면 좋다.

### 비지도 사전훈련

레이블이 없는 훈련 샘플을 모으는 것은 비용이 적게 들지만 레이블을 부여하는 것은 비싸다.
따라서 레이블 되지 않은 훈련 데이터를 ㅁ낳이 모아 오토인코더나 제한 볼츠만 머신과 같은 알고리즘으로 비지도 학습 모데릉ㄹ 훈렪나다. 
초기에는 층이 많은 모델을 훈련시키는 것이 어려워 탐욕적 층 단위 사전훈련을 사용하였다.
이는 먼저 하나의 층을 가진 비지도 학습 모델을 훈련한다. 그 다음 이 층을 동력하여 그 위에 다른 층을 추가한 다음에 모델을 다시 훈련한다. 그 후 새로운 층을 동결하고 그 위에 새로운 층을 추가하는 방식으로 반복한다.
오늘날에는 일단 전체 비지도 학습모델을 훈련하고 오토인코더나 gan을 사용하여 학습한다.

### 속도 향상

아주 큰 심층 신경망의 훈련속도는 심각하게 느리다.
훈련속도를 높일 수 있는 방법으로는 표준적인 경사하강 옵티마이저 대신 더 빠른 옵티마이저를 사용하는 것이다.

모멘텀 최적화
경사하강법은 이전 그레디언트가 얼마였는지를 ㅗ려하지 않는다.
하지만 모멘텀 최적화는 이를 중요하게 여겨 매 반복에서 현재 그레디언트를 모멘텀 벡터에 더하고 값을 빼는 방식으로 가중치를 갱신한다.
즉, 그레디언트를 속도가 아니라 가속도로 사용한다.
$ $
경사하강법은 가파른 경사에서는 빠르게 내려가지만 긴 골짜기 에서는 오랜 시간이 걸린다.
반면 모멘텀 최적화는 골짜기를 따라 최적점에 도달할 때 까지 점점 더 빠르게 내려간다.
이는 지역 최적점을 건너 뛰도록 하는데 도움이 된다.

네스테로프 가속 경사
$$
일반적으로 모멘텀 벡터가 최적점을 향하는 방향을 가리킬 것이기 때문에 이렇게 사용이 가능하다.
Adagrad
가파른 차원을 따라 그레디언트 벡터의 스케일을 감소하여 문제를 해결한다.
$$
첫번째 단계는 그레디언트의 제곱을 벡터에 누적한다.
따라서 비용함수가 가파르다면 s값이 반복이 진행될수록 커진다.
두번째 단계는 경사하강법과 동일하지만 그레디언트 벡터의 크기를$$로 조정한다.
따라서 경사가 완만한 구간에서는 학습률이 평범하게 감소하지만 경사가 가파른 곳에서는 급격하게 감소하게 된다.
즉, 적응적 학습률이 적용되어 전역 최적점으로 가도록 갱신하는데 도움이 된다.
다만, 이는 간단한 2차 방정식 문제에대해서는 잘 작동하지만 신경망을 훈련할 때 너무 일찍 멈추는 경우가 가끔 있다는 단점이 있다.

Rmsprop
Adagrad는 너무 빨리 느려져서 전역 최적점에 수렴하지 못하는 위험이 있다. 따라서 모든 그레디언트의 누적이 아닌 최근 반복에서 비롯된 그레디언트만 누적하는 알고리즘을 통해 이것을 해결하였다.
$$
감쇠율 베타는 0.9로 설정한다.

Adam 
적응적 모멘트 추정을 의미하는 것으로 모멘텀 최적화와 rmsprop의 아이디어를 합친것이다.
$$
$$
1,2,5단계를 보면 모멘텀 최적화와 rmsprop와 유사함을 확인 할 수 있다.
3, 43ㅏㄴ계에서는 m과s가 0으로 초기화 되기 때문에 훈련 초기에 0으로 치우치게 되는것을 줄여주는 역할을 한다.

### 학습률 스케줄링

학습률이 너무 크면 발산하고, 작으면 너무 느리게 학습한다.
그리고 발산하지 않더라도 학습률이 너무 크면 초반에 빠르게 수렴하지만 최적범에 수렴하는 것이 오래걸린다
학습률을 작은 값에서 큰 값 까지 증가시키면서 좋은 것을 찾는 방법이 제일 정확하겠지만 이는너무 많은 반복을 야기한다.

따라서 확습 스케줄을 사용해서 초반에는 큰 학습률로 시작했다가 학습 속도가 느려질 때 학습률을 낮춰 고정 학습률보다 좋은 솔루션으로빨리 수렴하게한다.

### 규제를 사용해 과대적합 피하기

심층 신경망은 전형적으로 수만개에서 때로는 수백만개의 파라미터를 가지고 있다.
때문에 네트워크의 자유도가 높아 대규모의 복잡한 데이터셋을 학습할 수 있다.
하지만 높은 다유도 때문에 과대적합 하기 쉬워 규제가 필요하다.

드롭아웃
드롭아웃은 심층 신경망에서 가장 인기 있는 규제 기법이다.
이는 매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃 될 확률p를 가진다. 즉, 이번 훈련 스텝에는 완전히 무시되지만 다음 스텝에서는 다시 활성화 될 수 있다. 이 방식으로 훈련하면 이웃한 뉴렌에 맞춰 적응할 수 없게 된다.따라서 가능한 자기 자신이 유용해지는 작용을 한다.
따라서 뉴런들이 몇 개의 입력 뉴런에만 지나치게 의존하는게 ㅔ아니라 모든 입력 뉴런에 주의를 기울여 입력값의 작은 노이즈에 덜 민감해진다.
또한 각 훈련 스텝에서 고유한 네트워크가 생성되기 때문에 같은 네트워크가 훈련동안 2번 선택될 가능성이 사실상 거의 없다.
결과적으로 만들어진 신경망은 이 모든 신경망을 평균한 앙상블로 볼 수 있다.

맥스-노름 규제
각각의 뉴런에 대해 입력의 연결 가중치가 $$ 이 되게 한다.

