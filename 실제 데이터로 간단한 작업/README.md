## 실제 데이터로 작업해보기

 ### 캘리포니아 인구조사 데이터를 사용해 캘리포니아의 주택 가격 모델을 만들자
   
 - 데이터 : 블록 그룹마다 인구, 중간소득, 중간주택가격등을 담고 있다.
   
 - 목표: 다른 측정 데이터가 주어졌을 때 그 구역의 중간주택가격을 예측
   
캘리포니아 인구조사 데이터는 레이블된 훈련 샘플이 있으므로 지도학습이고, 데이터에 따른 값을 구하는 것이므로 회귀문제이다.
그리고 데이터가 실시간으로 빠르게 변하는 것이 아니라 주어진것이므로 오프라인학습인 배치학습이다.

성능지표는 회귀문제의 경우 대표적인 지표인 평균제곱근오차를 사용한다

$$
 (평균 제곱근 오차) = \sqrt {{1 \over m}  \sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2 }
$$

다만 이상치가 많은 경우 평균 절대 오차를 사용하기도 한다

$$
 (평균 절대 오차) = {1 \over m}  \sum_{i=1}^m |h(x^{(i)}) - y^{(i)}| 
$$

## 데이터 정제 과정

 - 식별자의 colum이 없는 경우

   식별자 열이 없으면 행의 인덱스를 id로 사용하면 된다. 하지만 이러한 경우는 데이터셋에 데이터를 추가해야하는데 만약 이러한 것이 불가능 할 경우는 특성중에서 안전한 특성을 사용해야한다. <br>
   여기서 안전한 특성이라는 것은 예를들면 경도와 위도와 같이 아무리 시간이 지나더라도 변하지 않는 값들을 의미한다.
   
 - 데이터 셋을 나눌 때
    
   데이터 셋을 여러 서브 셋으로 나눌 때 데이터가 많은 경우에는 확률적으로 골고루 나눠질 확률이 높기 때문에 순수하게 무작위로 나누는 방법을 사용하여도 크게 문제가 없지만, 데이터의 수가 적을 경우 샘플링 편향이 생길 수 있다.<br>
   즉, 샘플링을 할 때 중요한 특징들은 전체 데이터의 비율에 맞게 추출해서 샘플의 편향을 줄여야 한다.

## 데이터 이해를 위한 탐색과 시각화

데이터를 시각화 하면 데이터 중에서 이상한 형태가 존재하는 것을 확인해 볼 수 있고, 학습을 할 때 이러한 부분들을 제거하여 모델이 이상한 형태의 학습을 하지 않도록 하는 효과가 있다.

머신러닝 알고리즘은 누락된 특성이 있을 경우
 1. dropna()를 이용해 해당 구역을 제거

 2. 특성에 누락된 것이 많을 경우는 drop()을 사용해 특성자체를 삭제

 3. 평균값등을 채우는 방법을 쓸 수 있다.

## 특성 스케일링

데이터의 특성에 따라서 가지는 값의 범위가 다른데 이것의 차이가 클 경우 알고리즘이 제대로 작동하지 않는다. 따라서 모든 특성의 범위를 같도록 만들어주는게 중요하다.

이때 min-max스케일링과 표준화 방법이 주로 사용된다

Min-max스케일링 : 0 ~ 1 범위에 값이 들어가도록 값을 이동하고 스케일을 조정하면 된다. 이는 데이터에서 최솟값을 뺀 다음에 최대값과 최솟값 범위의 길이로 나누면 된다.

표준화 방법 : 먼저 평균을 뺀 후 표준편차로 나누어 결과 분포의 분산이 1이 되도록 한다. 이는 min-max와는 다르게 상한과 하한이 없어서 특정알고리즘에는 문제가 될 수 있지만 이상치의 영향은 더 적게받는다.

## 모델 세부 튜닝 방법
 - 그리드 탐색 : GridSerchCV를 이용해서 탐색하고자 하는 하이퍼파라미터와 시도해볼 값을 지정하여 가능한 모든 하이퍼파라미터 조합에 대해 교차검증을 사용해서 평가하게 된다.<br>
다만 많은 수의 조합을 탐구할때는 시간이 매우 오래걸려서 비효울적이다.

 - 랜덤 탐색: 가능한 모든 조합을 시도하는 대신 각 반복마다 임의의 수를 대입해서 지정한 횟수만큼만 평가한다. <br>

## 모델의 유지 및 보수

모델을 다 만들었더라도 일정 간격으로 시스템의 실시간 성능을 체크하고 성능이 떨어졌을 때 알 수 있도록 모니터링 코드를 작성해야한다.<br>
왜냐하면 시간이 지나면서 데이터가 더 정밀해지고(예를 들어 카메라 발전으로 인해 사진 데이터가 더 정밀해진다.)더 좋은 모델이 나옴으로 오래전에 만들었던 내 모델은 낙후되는 경향이 있다.<br>
따라서 데이터가 계속 변화하면 데이터 셋을 업데이트 하고 모델을 다시 정기적으로 훈련해야한다.

