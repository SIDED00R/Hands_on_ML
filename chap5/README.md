## 서포트 벡터 머신(SVM)

svm분류기는 클래스 사이에 가장 폭이 넓은 도로를 찾는것으로 생각할 수 있는데, 이를 라지 마진 분류라고 한다.

svm에서 분류기의 결정 경계는 무수히 많이 고를 수 있다.
<br>
하지만 이중에서 샘플에 너무 가까운 것을 고르게 된다면 훈련 데이터로 분류를 할 때는 잘 작동할 수 있어도 새로운 데이터가 추가될 경우 오차가 크게 생길 확률이 높다.

 - 하드 마진 분류 : 모든 샘플이 도로 바깥쪽에 올바르게 분류되어있는 경우
   
   단점 : 데이터가 선형적으로 구분이 될 수 있어야 하고, 이상치에 매우 민감하게 반응한다.
   
이런 문제를 해결하기 위해 조금 더 유연한 소프트 마진 분류를 사용해야한다.
   
svm에서 C 매개변수는 모델의 정규화를 조절하는 역할을 한다.

여기서 C값이 크면 마진 오류를 허용하지 않고 가능한 많은 데이터를 정확하게 분류하려고 한다.<br>
그리고 C값이 작으면 마진은 더 크고 정확히 분류하기보다는 일반적인 패턴을 파악하려고 하는 경우에 사용된다.<br>
따라서 모델이 과대적합한 경우는 값을 줄여 모델을 규제할 수 있다.

## 비선형 svm 분류

특성을 추가하여 선형적으로 구분이 안되는 데이터셋을 선형으로 구분되게 변형하여 svm을 적용한다.

다만, 낮은 차수의 다항식은 복잡한 데이터셋을 잘 표현하지 못하고, 높은 차수의 다항식은 매우 많은 특성을 추가해야 하기 때문에 모델이 느려지게된다.

따라서 커널 트릭을 통해 실제로 특성은 추가하지 않으면서 특성을 많이 추가한 효과를 얻어야한다.

### 커널트릭

커널 함수를 통해 두 데이터간의 내적을 계산하여 고차원 공간으로 변환을 한다.

커널은 변환 파이를 계산하지 않고 원래 벡터 a와 b에 기반한여 내적을 계산할 수 있는 함수이다. 

 - 선형 $K(a, b) = a^T b$
   
 - 다항식 : $K(a, b) = ( \gamma a^T b + r)^d$
  
 - 가우시안: $K(a, b) = exp(- \gamma \lVert a - b \rVert ^2 )$
   
 - 시그모이드: $K(a, b) = \tanh (\gamma a^T b + r)$
   
등의 커널이 존재한다.

이는 머서의 조건(K가 매개변수에 대해 연속, 대칭)을 만족할 때 매핑함수 파이가 존재한다.

### 유사도 특성

비선형 특성을 다루는 다른 기법은 각 샘플이 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도 함수로 계산한 특성을 추가하는 것이다.

랜드마크를 설정하는 방법은 간단히 데이터 셋에 있는 모든 샘플 위치에 랜드마크를 설정하는 것이다. <br>
이러면 차원이 매우 커져 훈련세트가 선형적으로 구분될 가능성이 높아진다.

다만 훈련세트가 매우 클 경우 아주 많은 특성이 만들어지는 단점이 있다.

유사도 함수로는 가우시안 방사 기저함수(RBF)를 주로 사용한다.
RBF는 감마값을 조절하여 샘플을 분류하는데, 감마값을 증가시키면 가우시안 그래프가 좁아져서 각 샘플의 영향범위가 줄어들고, 작은 감마값을 사용하면 종 모양이 넓어져 샘플이 넓은 범위에 영향을 줘 결정 경계가 더 부드러워진다.

### svm회귀

분류를 사용할 때는 마진안에서 두 클래스 간의 도로폭이 가능한 최대가 되도록 하였다.

하지만 회귀를 사용할 때는 제한된 도로 안에서 가능한 많은 샘플이 도로위에 들어가도록 학습을 한다.


### 결정 함수와 예측

svm분류기 모델은 단순히 결정함수를 계산해서 새로운 샘플X의 클래스를 예측한다.

이때 결괏값이 0보다 크면 예측된 클래스y는 양성 클래스로, 그렇지 않으면 음성클래스로 분류한다.

그리고 훈련은 가능한 마진을 크게하는 w와 b를 찾는 것이다.

이때 가중치 벡터 w가 작을수록 마진은 더 커진다.

