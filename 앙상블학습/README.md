## 앙상블 학습

### 투표기반 분류기

- 직접 투표 분류기 : 각 분류기의 예측을 모아서 가장 많이 선택한 클래스로 예측하는 것이다.
  이렇게 하면 한개의 분류기를 사용했을 때 보다 더 좋은 분류기를 만들 수 있다.
  
  다만 이는 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없을 때 의미가 있는것이다.
  
  즉, 같은 데이터로 훈련을 시킬 경우 같은 종류의 오차가 만들어지기 쉽기 때문에 잘못된 클래스가 다수인 경우가 많고, 앙상블의 정확도가 낮아지게된다.

 - 간접 투표 분류기 : 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균내어 확률이 가장 높은 클래스를 예측하는 방법이다.

   이는 확률이 높은 투표에 비중이 더 높기 때문에 직접투표 방식보다 성능이 더 높다.

다양한 분류기를 만들기 위해 서로 다른 훈련 알고리즘을 사용할수도 있지만, 같은 알고리즘을 사용할 수도 있다.

동일한 알고리즘으로 다양한 분류기를 만들기 위해서는 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 방식을 사용해야한다.

 - 배깅 : 훈련 세트에서 중복을 허용하여 샘플링
   
   배깅을 사용하면 어떤 샘플은 여러 번 선택되고 다른 것은 한번도 선택되지 않을 수도 있다.
   
   m개의 샘플에서 m개를 선택 : $1 - \lim\limits_{m \to \infty} (1 - \frac{1}{m})^m = 1 - e^{-1} \approx 0.63$

   이는 m개의 샘플에서 m개를 선택하면 약 63%만 선택된다는 것을 의미한다.
   
   그러면 남는 37%의 샘플이 있는데 이를 oob(out of bag)샘플이라고 하고 이는 훈련에 사용되지 않음으로 검증세트로 사용할 수 있다.
   
 - 페이스팅 : 훈련 세트에서 중복을 허용하지 않고 샘플링
   
배깅과 페이스팅의 장점: 모두 동시에 다른 cpu코어나 서버에서 병렬로 학습이 가능해 시간 효율이 좋다.

각각의 예측기는 편향이 크지만 수집함수(분류일때는 일반적으로 통계적 최빈값)를 거치면서 편향과 분산이 모두 감소하게 된다.

### 랜덤 포레스트

랜덤포레스트는 일반적으로 배깅 방법을 적용한 결정 트리의 앙상블이다.

랜덤 포레스트는 트리의 노드를 분할할 때 전체 특성중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입한다.

이는 트리를 다양하게 만들고 편향을 손해보는 대신 분산을 낮춰 전체적으로 훌륭한 모델을 만든다.

 - 익스트림 랜덤 트리 : 최적의 임곗값을 찾는 대신 후보 특성을 이용하여 무작위로 분할한 다음 최상의 분할을 선택하는 극단적으로 무작위한 트리이다.

랜덤 포레스트의 장점: 어떤 특성을 사용한 노드가 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정하기 쉽다.

특성의 중요도 : 노드에 사용된 특성별로 $(현재 노드의 샘풀 비율 * 불순도) – (왼쪽자식노드의 샘플 비율 * 불순도 + 오른쪽자식노드의 샘플비율 * 불순도)$를 계산하여 더하고, 특성의 중요도 합이 1이 되도록 전체 합으로 나누어 정규화한다.

### 부스팅

약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법이다

 - 에이다 부스트 : 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 방법

 - 에이다 부스트의 훈련 방법
   
   1. 알고리즘의 기반이 되는 첫번째 분류기를 훈련세트에 훈련시키고 예측을 만든다.
  
   2. 알고리즘이 잘못 분류되 훈련 샘플의 가중치를 상대적으로 높이고, 두 번째 분류기는 업데이트된 가중치를 사용하여 훈련세트를 훈련하고 다시 예측을 만든다.
  
   3. 이 과정을 반복한다.

 - 그레디언트 부스팅 : 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시켜 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다.
   
   그레디언트 부스팅에서 Learning_rate 매개변수는 각 트리의 기여 정도를 조절한다.

   만약 Learning_rate가 낮게 설정하면 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 예측의 성능은 좋아진다.<br>
   이는 축소(shrinkage)라고하는 규제 기법이다.

   다만 너무 많은 트리의 수가 생길 수 있어 조기 종료 기법을 활용하면 최적의 트리 수를 찾을 수 있다.
   
 - 확률적 그레디언트 부스팅 : 각 트리가 무작위로 선택된 훈련 샘플로 학습을 한다.
 
   이는 편향이 높아지는 대신 분산이 낮아지고, 훈련 속도가 빨라지게 된다.

### 스태킹

앙상블에 속한 모든 예측기의 예측을 취합하는 함수를 사용해 예측기들이 각기 다른 값을 받아 마지막 예측기(블렌더 혹은 메타학습기)를 통해 최종 예측을 만드는 방식이다.

블렌더를 학습시키는 일반적인 방법은 홀드아웃 세트를 사용하는 것이다.

이는 먼저 훈련세트를 두개의 서브셋으로 나눠 첫번째 서브셋은 첫번째 레이어의 예측을 훈련시키기 위해 사용된다. 

그 후 첫번재 레이어의 예측기를 사용하여 두 번째 세트에 대한 예측을 만드는데 이때 두 번째 서브셋 데이터들은 기존에 사용된적이 없기 때문에 새로운 예측이 나올것이다.

마지막으로 두번째 레이어는 첫 번째 수준 모델들의 예측 결과를 기반으로 최종 예측을 수행하는 역할을 한다.

