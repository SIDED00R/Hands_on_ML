## 강화학습

강화 학습에서 소프트웨어 에이전트는 관측을 하고 주어진 환경에서 행동을 하여 결과로 보상을 받는다.

에이전트는 보상의 기대치를 최대로 만드는 행동을 학습하는것이다.

이때 양의 보상이 없이 음의 보상만 있을수도 있는데 예를들면 미로같은경우 미로안에 있는 시간이 증가할수록 음의 보상의 계속 받는 방식이다.

에이전트가 행동을 결정하기 위해 정책이라는 알고리즘을 사용하는데 정책이 꼭 결정적일 필요없이 무작위성이 포함된 확률적 정책일수도 있다.

이런 정책을 훈련시키기 위해 정책 파라미터를 학습해야하는데 가장 단순한 방법은 많은 값을 시도해 보고 가장 성능이 좋은 조합을 고르는 학습 알고리즘이 있다.

이러한 정책 탐색은 정책 공간이 매우 큰 경우 시간이 많이 걸리기 때문에 좋지 못한 방법이다.

다른 알고리즘 방법으로는 유전 알고리즘이 있다.

유전 알고리즘은 처음에 무작위로 정책을 랜덤하게 생성한 후 성능이 낮은 정책은 버리고, 성능이 좋은 정책을 기반으로 약간의 무작위성을 더해 자식 정책을 만든다.

이러한 과정을 좋은 정책을 찾을 때까지 반복하는 것이다.

다른 방법으로는 정책 그레디언트가 있는데 이는 보상의 그레디언트를 평가해 높은 보상의 방향을 따르는 그레디언트로 파라미터를 수정하는 최적화 기법이다.

정책 그레디언트 방식중에 인기있는 알고리즘은 REINFORCE 알고리즘으로 신경망 정책이 여러 번에 걸쳐 게임을 실행하고, 각 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그레디언트를 계산한다.

그리고 에피소드를 몇 번 실행 후 각 행동 이익을 계산하여 행동의 이익이 양수이면 미래에 선택 될 가능성이 높도록 앞서 계산한 그레디언트를 적용하고, 음수인 경우는 반대의 그레디언트를 적용한다.

마지막으로 모든 결과 그레디언트 벡터를 평균내서 경사하강법 스텝을 구한다.

하지만 이런 방식은 매우 긴 시간동안 게임을 해야 정책을 많이 개선할 수 있어 샘플 효율성이 좋지 못하다.

따라서 액터-크리틱과 같은 더 강력한 알고리즘을 사용하면 좋다.

### 신경망 정책

신경망 정책은 신경망이 관측을 입력으로 받고 각 행동에 대한 확률을 추정하는 방식이다.

이때 가장 높은 점수의 행동을 바로 하는 것이 아니라 확률을 기반으로 선택을 하는 이유는 이러한 방식이 에이전트가 새로운 행동을 탐험 하는 것과 잘 할 수 있는 행동을 활용하는 것 사이에 균형을 맞추기 때문이다.

### 행동 평가: 신용 할당 문제

각 스텝에서 가장 좋은 행동이 무엇인지 안다면 추정된 확률과 타깃 확률 사이의 크로스 엔트로피를 최소화 하도록 신경망을 훈련할 수 있다.

하지만 강화학습에서 가이드는 보상뿐인데 이 보상이 일반적으로 드물고 지연되서 나타난다.

따라서 결과에 대한 모든 행동의 원인이 가장 마지막 행동 때문이라고는 할 수 없다.

이를 신용 할당 문제라고 하는데 간단히 말해 에이전트가 받은 보상의 출처를 알기 어렵다는 것이다.

이를 해결하기 위한 방법은 행동이 일어난 후 각 단계마다 할인 계수를 적용한 보상을 합하여 행동을 평가하는 것이다.

할인계수가 0에 가까우면 미래의 보상은 현재의 보상만큼 중요하게 생각하지 않을것이다.


### 마르코프 결정 과정(MDP)

마르코프 속성 : 과거의 상태에 대한 정보 없이 현재 상태만으로 미래 상태를 예측할 수 있다.

마르코프 연쇄는 정해진 개수의 상태를 가지고 각 스텝마다 한 상태에서 다른 상태로 랜덤하게 전이되고 이때 전이되기 위한 확률은 고정되있을 때 과거의 상태와 무관하게 오직 전이되는 쌍에만 의존한다는 것이다.

MDP은 이와 비슷하지만 전이 확률이 고정되지 않고 선택된 행동에 따라 달라지고,  어떤 상태전이는 보상을 반환하여 에이전트가 보상을 최대화 하기 위한 정책을 갖는 차이가 있다.

어떤 상태 s의 최적의 상태가치 V(s)를 추정하는 방법으로 에이전트가 상태 s에서 최적으로 행동한다고 가정하고 평균적으로 기대할 수 있는 할인된 미래 보상을 합하면 된다.

에이전트가 최적으로 행동하면 $V^* (s) = \underset{a}\max \sum\limits_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*(s')] \quad \text{for all }s$ 이와 같은 벨먼 최적 방정식이 적용된다.

$T(s, a, s')$ : 에이전트가 행동 a를 선택했을 때 상태 s 에서 s'로 전이될 확률

$R(s, a, s')$ : 에이전트가 행동 a를 선택해서 상태 s 에서 s'로 이동했을 때 에이전트가 받을 수 있는 보상

$\gamma$ : 할인 계수

이 식은 에이전트가 최적으로 행동하면 현재 상태의 최적 가치는 하나의 최적 행동으로 인해 평균적으로 받게 될 보상과 이 행동이 유발할 수 있는 가능한 모든 다음 상태의 최적 가치의 기대치를 합한것과 같다는 뜻이다.

최적의 가치를 알면 좋겠지만 이를 직접적으로 알 수 없기 때문에 최적의 상태 행동 가치를 주정할 수 있는 Q-가치를 사용하면 된다.

Q-가치는 에이전트가 상태 s에 도달해서 행동 a를 선택한 후 행동의 결과를 얻기 전에 평균적으로 기대할 수 있는 할인된 미래 보상의 합이다.

여기서는 에이전트가 행동 이후에 최적으로 행동할 것을 가정한다.

### 시간차 학습

독립적인 행동으로 이루어진 강화학습은 마르코프 결정 과정으로 모델링 되지만 초기에 에이전트가 전이 확률을 알지 못하고, 보상이 얼마나 되는지도 모른다.

따라서 적어도 한 번은 각 상태와 전이를 경험해야한다.

시간차 학습은 에이전트가 MDP에 대해 일부정보만 알고있을 때 다룰 수 있도록 변형한 것으로 에이전트가 초기에 가능한 상태와 행동만 알고 다른 것은 모른다고 가정한다.

에이전트는 탐험 정책을 사용해서 MDP를 탐험하는데 탐험이 진행될수록 TD학습 알고리즘이 실제로 관측된 전이와 보상에 근거하여 상태 가치의 추정값을 업데이트한다.

TD 학습 알고리즘 : $V_{k+1} (s) \gets (1-\alpha) V_k (s) + \alpha (r + \gamma . V_k(s'))$

$r + \gamma . V_k(s')$ : TD 타깃

### Q-러닝

Q-러닝 알고리즘은 전이 확률과 보상을 초기에 알지 못한 상황에서 Q-가치 반복 알고리즘을 적용한 것으로 에이전트가 행동하는 것을 보고 점진적으로 Q-가치 추정을 향상하는 방식으로 작동한다.

Q-러닝 알고리즘 : $Q_{k+1}(s, a) \gets (1-\alpha)Q_k(s,a) + \alpha(r + \gamma . \underset{a'}{\max} \, Q_k(s', a'))$

이는 각 상태-행동 쌍마다 알고리즘이 행동 a를 선택해 상태 s를 떠났을 때 에이전트가 받을 수 잇는 보상 r과 기대할 수 있는 할인된 미래 보상의 합을 더한 이동 평균을 저장한다.

훈련정책을 반드시 실행에 사용하지 않기 때문에 오프 폴리시 알고리즘이라고도 한다.

### 탐험 정책

Q-러닝에서 탐험 정책이 MDP를 충분히 탐험해야 작동하는데 모든 상태와 전이를 경험하려면 시간이 매우 오래 걸리게 된다.

따라서 ε-그리디 정책을 사용하여 각 스텝에서 ε 확률로 랜덤하게 행동하거나 1-ε 확률로 그 순간 가장 최선인 행동을 해야한다.

이것의 장점은 Q-가치 추정이 점점 향상되기 때문에 환경에서 관심있는 부분을 살피는데 점점 더 많은 시간을 사용한다는 점이다.

다른 방법으로는 탐험의 가능성에 의존하는 대신 이전에 많이 시도하지 않았던 행동을 시도하도록 탐험 정책을 강조하는 방식이 있다.

이는 Q-가치에 보너스를 추가하는 방식으로 구현된다.

탐험 함수를 사용한 Q-러닝 : $Q(s, a) \gets (1-\alpha)Q(s,a) + \alpha\left(r + \gamma \, \underset{a'}{\max}f(Q(s', a'), N(s', a'))\right)$

Q-러닝의 주요 문제는 많은 상태와 행동을 가진 대규모의 MDP에 적용하기 어렵다는 것이다.

왜냐하면 대규모 상태-행동 공간에서 Q-러닝은 각 상태-행동 쌍에 대한 Q-값을 업데이트해야 하는데, 이러한 업데이트는 계산적으로 많은 리소스를 요구하여 비용이 많이 발생하기 때문이다.

따라서 규모가 큰 경우 모든 Q-가치에 대한 추정값을 기록할 수 없기 때문에 상태-행동 쌍의 Q-가치를 근사하는 함수를 적절한 개수의 파라미터를 사용해 찾는 근사 Q-러닝을 사용하면 된다.

그리고 심층 신경망을 사용해 복잡함 문제에 더 좋은 결과를 보이는 심층 Q-러닝이 있는데 이는 심층 Q-네트워크(DQN) 알고리즘에 관련된 기술을 포함하여 실제 환경에서의 안정성을 보장하는 데 더 적합한 형태이다.

### 심층 Q-러닝의 변종

-	고정 Q-가치 타깃

 	기본 심층 Q-러닝 알고리즘에서 모델은 예측을 만들고 타깃을 설정하는데 모두 사용되어 발산, 진동, 동결과 같은 문제가 발생하여 네트워크를 불안정하게 만든다.

 	이를 해결하기 위해 두개의 DQN을 사용해서 첫 번째 DQN는 각 스텝에서 학습하고 에이전트를 움직이는데 사용하는 온라인 모델이고, 두 번째 DQN는 타깃을 정의하기 위해서만 사용되는 타깃 모델이다.

 	타깃모델이 온라인 모델보다 자주 업데이트 되지 않으므로 Q-가치 타깃이 더 안정직이며 앞서 언급한 피드백 반복을 완화하고 이에 대한 영향이 감소된다.

-	더블 DQN

 	타깃 네트워크가 Q-가치를 과대평가 하기 쉽다는 관측을 기반으로 한다.

 	이러한 문제를 개선하기 위ㅐㅎ 다음 상태에서 최전의 행동을 선택할 때 타깃 모델 대신 올ㄴ라인 모델을 사용하도록한다.

-	우선순위 기반 경험 재생

 	재생 버퍼에서 경험을 균일하게 샘플리하는 것이 아니라 중요한 경험을 더 자주 샘플링하는 방식이다

 	즉, 학습 진행을 빠르게 만들면 중요한 경험을 더 중요한것으로 간주하는데
 	
-	듀얼링 DQN
상태-행동 쌍의 Q-가치가 ()처럼 표현될 쉬 있는데 
듀얼링 dqn에서는 모델이 상태의 가치와 가능항 각 행동의 이익을 모두 추정하여 최선의 행동은 이익이 0이기 때문에 모델이 예측한 모든 이익에서 모든 최대 이익을 빼서 구한다.

이 밖의 유명한 강화 학습 알고리즘
-	액터 크리틱 알고리즘
정책 그레디언트와 심층 Q-네트워크를 결합한 강화 학습 알고리짐으로 정책 네트워크와 DQN2개를 포함한다.
-	A3C
비동기적인 일정한 간격으로 각 에이전트가 마스터 네트워크로 가중치 업데이트를 보내고 네트워크에서 최신의 가중치를 받온다.
이때 각 에이전트는 마스터 ㅇ네크워크의 향상에 기여하면서 다른 에이전트가 학습한 것에서 혜책을 받는다.
-	A2C
비동기성을 제거한 A3C알고리즘이다.
모든 모델이 동기적으로 업데이트 되어 GPU의 성능을 최대한 활용해 큰 배치에 대해 그레디언트 업데이트를 수행할 수 있다.
-	SAC
모델이 보상 뿐만 아니라 행동의 엔트로피를 최대화 하도록 학습한다.
즉, 가능한 많은 보상을 얻으려 하지만 간으한 예측이 어렵도록 만들어 DQN이 불완전한 추정을 만드는 행동을 반복해 실행하지 않도록 돕는다.
-	호기심 기반 탐색
강화 학습에서 계속 발생하는 문제는 희소성인데 이는 학습을 느리고 비효율적으로 만든다.
이를 해결하기 위해 보상을 무시하고 에이전트가 순수 호기심 만으로 환경을 탐색하여 보상이 환경에서 오는 것이 아니라 에이전트 자체의 성질이 되는 방식이다.
따라서 에이전트가 예측 가능해지면 다른 곳으로 이동하고 이또한 예측 가능해지면 다른곳으로 이동하는 과정을 반복한다.
게임의 경우 이겼을 때 보상은 없지만 졌을 때 게임이 다시 시작되는건 예측이 가능함으로 이를 피하기 위해 학습을 한다.

