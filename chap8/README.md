## 차원 축소

### 차원의 저주

고차원 데이터의 경우 많은 공간을 가지고 있기 때문에 훈련 데이터가 서로 멀리 떨어져있어 데이터셋이 매우 희박할 가능성이 높다.

이는 새로운 샘플로도 훈련샘플과 거리가 멀다는 뜻으로 예측을 위해 더 많은 추정을 해야하기 때문에 저차원일때보다 예측이 불안정해진다는 의미이다.

즉, 훈련세트에 과대적합할 위험이 커진다는 것이다.

물론 훈련 샘플의 밀도가 충분히 크다면 이러한 일이 없겠지만 데이터를 무한히 얻을 수 없고, 비용또한 많이 들기 때문에 이러한 방법을 사용할 수는 없다.

### 차원 축소 접근법

-	투영 : 대부분의 데이터들은 모든 차원에 균일하게 퍼져있는데, 이때 어떤 특성끼리는 서로 강하게 연관이 되어있지만 특별히 변화가 없는 특성들도 많이 존재한다.

 	결과적으로 모든 훈련 샘플이 고차원 공간 안에서 저차원의 부분공간에 놓여있다는 뜻으로 해석할 수 있다.

  따라서 이런 저차원의 부분 공간으로 데이터를 투영하여 차원을 낮춰 차원 축소를 하는 방법이다.

-	매니폴드 학습 : 고차원 공간에서 휘어져있거나 뒤틀려져있는 형태의 데이터 분포들은 투영으로 표현하기 힘든데 이때 사용하는 것이 매니폴드 방식이다.

 	많은 축소 알고리즘은 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여있다는 매니폴드 가정에 근거하여 차원 축소 알고리즘이 작동된다.

다만 모델을 훈련시키기 전에 훈련 세트의 차원을 감소시키면 훈련 속도는 빨라지지만 항상 더 좋거나 간단한 솔루션이 되는 것은 아니다.

### PCA

PCA 즉, 주성분 분석은 데이터에 가장 가까운 초평면을 정의한 다음에 데이터들을 이 평면에 투영시키는 방식을 사용한다.

투영을 할때는 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되어 좋다.

즉, 원본 데이터와 투영된 데이터 사이의 평균 제곱 거리가 최소화 되는 축을 찾는 것이 중요하다.

 - PCA의 훈련 방법
   
   1. 먼저 훈련 세트의 분산이 최대인 축을 찾는다.
      이때 훈련세트의 주성분을 찾는 방법은 특잇값 분해를 이용해 표준 행렬 분해기술로 찾으면 된다.
      
   2. 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는다.
      
   3. 이전의 축에 직교하는 다음 축을 데이터셋에 있는 차원의 수 만큼 반복하여 찾는다.
  
   4. 주성분을 모두 찾으면 처음 d개의 주성분으로 구성된 초평면에 투영하여 데이터셋의 차원을d차원으로 축소한다.

#### 적절한 차원 수 선택하기

축소할 차원의 수를 임의로 정하기 보다는 충분한 분산이 될 때까지 더해야 할 차원의 수를 선택하는 것 좋다.

혹은 설명된 분산을 차원수에 대한 함수로 표현하여 분산의 빠른 성장이 멈추는 변곡점을 택하면 된다.

물론 모델을 훈련하는 것이 아니라 시각화를 위해 차원을 축소하는 경우는 2개나 3개의 차원으로 줄이는 것이 좋다.

압축을 하고 나면 데이터셋의 크기가 줄어들어 알고리즘의 속도를 크게 높일 수 있다.

그리고 완벽하게는 아니지만 원본 데이터와 매우 유사하게 다시 복원도 할 수 있는데 이때 발생하는 오차를 재구성 오차라고 한다.

PCA의 문제점은 svd알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 한다는 것이다.

이러한 문제를 해결하기 위해 점진적PCA(IPCA)알고리즘을 사용하면 된다.

IPCA는 훈련 세트를 미니배치로 나눈 뒤 알고리즘에 하나씩 주입하는데, 이렇게 하면 새로운 데이터가 준비되는대로 실시간으로 PCA를 적용할 수 있게 되는 장점이 있다.

### 커널 PCA

커널 트릭과 같은 기법을 PCA에 적용하여 차원 축소를 위한 복잡한 비선형 투형을 수행하는 것이다.

이는 투영된 후에 샘플의 군집을 유지하거나 꼬인 매니폴드에 가까운 데이터셋을 펼칠때도 유용하다.

커널PCA는 비지도학습이기 때문에 좋은 커널과 하이퍼파라미터를 선택하기 위한 명확한 기준이 없다.

따라서 그리드 탐색을 활용하여 주어진 문제에서 성능이 가장 좋은 커널과 하이퍼 파라미터를 선택할 수 있다.

다만, 재구성 방식은 선형PCA보다 조금 복잡한데 축소된 공간에 있는 샘플에 대해 선형PCA를 재구성하면 데이터가 원본 공간이 아닌 특성 공간에 놓이게 된다.

이때 이 특성 공간은 무한차원이기 때문에 이 점을 계산 할 수 없고, 결과적으로 실제 에러를 계산할 수 없다.

하지만 재구성된 포인트에 가깝게 매핑된 원본 공간의 포인트를 찾을 수 있으므로 이 재구성의 원상을 얻어 원본 샘플과의 제곱 거리 오차를 측정하여 이를 최소화 하는 커널과 하이퍼 파라미터를 선택하면 된다.

### LLE

LLE는 지역 선형 임베딩으로 비선형 차원 축소 기술중 하나이다.

이는 데이터의 지역적 구조를 보존하기 때문에 주로 매니폴드 문제에 적합하다.

LLE는 먼저 각 훈련 샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지를 측정하고, 국부적인 관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾는다. 

이때 이웃을 찾는 방법은 주로 k-nearest neighbors와 같은 방법을 사용합니다

### 다른 차원 축소 기법

-	랜덤 투영: 랜덤한 선형 투영을 사용해 데이터를 저차원 공간으로 투영한다.

-	다차원 스케일링: 샘플간의 거리를 보존하면서 차원을 축소한다.

-	Isomap : 각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만들고 샘플 간 지오데식 거리를 유지하면서 차원을 축소한다

-	t-SNE: 비슷한 샘플은 가까이하고 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소한다.
  이는 주로 시각화에 많이 사용되어 고차원 공간에 있는 샘플의 군집을 시각화 할 때 사용된다.

