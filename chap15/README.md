## RNN

RNN은 순환 신경망으로, 입력층에서 출력층까지 한쪽 방향으로만 흐르는 피드 포워드 신경망과 매우 비슷하지만, 뒤쪽으로 순환하는 연결도 존재하는 차이가 있다.

RNN은 이전 시간 단계의 출력을 현재 시간 단계의 입력으로 사용하여 순차 데이터를 처리하는데 이러한 반복 구조는 순차 데이터에 대한 내부 상태나 메모리를 유지하도록 한다.

따라서 자연어 처리(NLP), 음성 인식, 시계열 예측 등과 같은 순차 데이터 관련 작업에 유용한 방식이다.

### 순환 뉴런

입력을 받아 출력을 만들고 자신에게도 출력을 보내는 뉴런 하나로 구성된 가장 간단한 형태의 RNN모델을 생각해보면 이 순환 뉴런은 타임 스텝 t에서 입력값 $x_{(t)}$ 와 이전 타임스텝 출력인 $y_{(t - 1)}$ 를 입력으로 받는다.

이때 각 순환 뉴런은 2개의 가중치를 가지는데 하나는 입력값 $x_{(t)}$ 를 위한것이고 다른 하나는 이전 타임 스텝의 출력인 $y_{(t - 1)}$ 를 위한것이다.

그러면 하나의 샘플에 대한 순환층의 출력은 다음과 같이 쓸 수 있다

$y_{(t)} =  \phi (W_x^T x_{(t)} + W_y^T y_{(t - 1)} + b)$

여기서 $\phi$ 는 ReLU와 같은 활성화 함수이다.

이를 미니배치에 있는 전체 샘플에 대한 순환 뉴런 층으로 확대하면 다음과 같다.

$Y_{(t)} = \phi (X_{(t)} W_{x} + Y_{(t-1)} W_{y} + b)$

### 메모리 셀

타임스텝 t에서 순환 뉴런의 출력은 이전 타임 스텝의 모든 입력에 대한 함수이므로 일종의 메모리 형태라고 말할 수 있다.

타임 스텝에 걸쳐 어떤 상태를 보존하는 신경망의 구성요소를 메모리 셀이라고 한다.

### 입출력 시퀀스

 - Sequence-to-Sequence network : 입력 시퀀스를 받아 출력 시퀀스를 만드는 것으로 주식 가격과 같은 시계열 데이터를 예측하는데 주로 사용된다.

 - Sequence-to-vector networ : 입력 시퀀스를 네트워크에 주입하고 마지막을 제외한 모든 출력을 무시하는 것으로 댓글이 긍정적인지 부정적인지 연속적인 단어의 입력을 통해 점수를 출력하는 것이다.

 - vector-to-Sequence networ : 각 타임 스텝에서 하나의 입력 벡터를 반복해서 주입하고 하나의 시퀀스 출력하는 것이다.

그리고 인코더라고 부르는 시퀀스 투 벡터 네트워크 뒤에 디코더라고 부르는 벡터 투 시퀀스 네트워크를 연결할수도 있다.

이는 마지막 입력값이 첫번째 결과값에 영향을 미칠 수 있기 때문에 더 좋은 예측이 가능해진다.

### RNN훈련하기

Bptt : Rnn을 훈련할때는 타임 스텝으로 네트워크를 펼치고 역전파를 이용한다.
시계열 데이터 : 데이터가 타임 스텝마다 하나 이상의 값을 가진 시퀀스로 타임스텝마다 하나의 값을 가지면 단변량 시계열, 여러값을 가지면 다변량 시계열이라고 한다.

가중 이동 평균이나 자동 회귀 누적 이동 평균과 같이 시계열을 예측하는 다양한 방법이 있다.
하지만 이런 방법ㅂ중에서는 트렌드나 계절성의 제거해야하는 것이 있다.

긴 시퀀스로 RNN을 훈련하면 많은 타임 스텝에 걸쳐 실행해야 함으로 RNN이 매우 깊어져 그레디언트 소실 혹은 폭주 문제가 발생 할 수 있다.

불안정한 그레디언트 문제

심층 신경망에서 사용했던 기법을 사용해서 그레디언트 문제를 완화할 수 있다.
하지만 ReLU와 같은 수렴하지 않는 활성화 함수는 지양하는 것이 좋은데 왜냐하면 경사 하강법이 출력을 조금 증가시키는 방향으로 가중치를 업데이트 한다고 했을 때 동일한 가중치가 모든 타임 스텝에서 사용되기 때문에 계속 출력이 증가하게 되어 폭주가 발생할 수 있기 때문이다.

Rnn에는 심층 피드포워드 네트워크처럼 배치 정규화를 사용하면 큰 성능의 향상이 없다.?!
그래서 rnn에 맞는 층 정규화를 사용해야한다.
층 정규화는 배치 차원에 대해 정규화하는 대신 특성 차원에 대해 정규화 하낟.
이는 샘플에 독립적으로 타임스텝마다 동적으로 필요한 통계를 계산할 수 있어 좋다.

단기 기억문제 해결하기
Rnn을 거치면 데이터가 변환되므로 일부 정보는 매 훈련 스텝후 사라진다.
따라서 어느정도 시간이 지나면 rnn에는 첫 번째 입력이 거의 없게 되는데 이런 문제를 해결하기 위해 장기 메모리를 가진 셀을 사용해야한다.
Lstm셀
장단기 메모리 셀은 네트워크가 장기 상태에 저장할 것, 버릴것, 읽어들일것을 학습한다.
장기 기억 네트워크에서 삭제 게이트를 지나면 일부 기억을 잃고 덧셈 연산으로 입력 게이트에서 선택한 기억을 일부 추가한다.
즉, 타임스텝마다 일부 기억이 삭제되고 일부 기억이 추가된다.

GRU셀
게이트 순환 유닛셀로  lstm의 간소화된 버전이다.

다만 이러한 셀을 사용하더라도 매우 제한적인 단기 기억을 가져 100타임스텝 이상의 시퀀스에서 장기 패턴을 학습하는데 어려움이 있다.

이를 해결하기 위해 1차원 합성곱을 사용해 입력 시퀀스를 짧게 줄이는 방법을 사용하면 된다.

Wavenet
층마다 팽창비율을 두배로 늘리는 1차원 합성곱 층을 쌓는다. 이렇게 하면 하위층은 단기 패턴을 학습하고 상위층은 장기 패턴을 학습아혀 아주 긴 시퀀스를 효율적으로 처리할 수 있다.

