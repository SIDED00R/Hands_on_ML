## RNN

RNN은 순환 신경망으로, 입력층에서 출력층까지 한쪽 방향으로만 흐르는 피드 포워드 신경망과 매우 비슷하지만, 뒤쪽으로 순환하는 연결도 존재하는 차이가 있다.

RNN은 이전 시간 단계의 출력을 현재 시간 단계의 입력으로 사용하여 순차 데이터를 처리하는데 이러한 반복 구조는 순차 데이터에 대한 내부 상태나 메모리를 유지하도록 한다.

따라서 자연어 처리(NLP), 음성 인식, 시계열 예측 등과 같은 순차 데이터 관련 작업에 유용한 방식이다.

### 순환 뉴런

입력을 받아 출력을 만들고 자신에게도 출력을 보내는 뉴런 하나로 구성된 가장 간단한 형태의 RNN모델을 생각해보면 이 순환 뉴런은 타임 스텝 t에서 입력값 $x_{(t)}$ 와 이전 타임스텝 출력인 $y_{(t - 1)}$ 를 입력으로 받는다.

이때 각 순환 뉴런은 2개의 가중치를 가지는데 하나는 입력값 $x_{(t)}$ 를 위한것이고 다른 하나는 이전 타임 스텝의 출력인 $y_{(t - 1)}$ 를 위한것이다.

그러면 하나의 샘플에 대한 순환층의 출력은 다음과 같이 쓸 수 있다

$y_{(t)} =  \phi (W_x^T x_{(t)} + W_y^T y_{(t - 1)} + b)$

여기서 $\phi$ 는 ReLU와 같은 활성화 함수이다.

이를 미니배치에 있는 전체 샘플에 대한 순환 뉴런 층으로 확대하면 다음과 같다.

$Y_{(t)} = \phi (X_{(t)} W_{x} + Y_{(t-1)} W_{y} + b)$

### 메모리 셀

타임스텝 t에서 순환 뉴런의 출력은 이전 타임 스텝의 모든 입력에 대한 함수이므로 일종의 메모리 형태라고 말할 수 있다.

타임 스텝에 걸쳐 어떤 상태를 보존하는 신경망의 구성요소를 메모리 셀이라고 한다.

### 입출력 시퀀스

 - Sequence-to-Sequence network : 입력 시퀀스를 받아 출력 시퀀스를 만드는 것으로 주식 가격과 같은 시계열 데이터를 예측하는데 주로 사용된다.

 - Sequence-to-vector networ : 입력 시퀀스를 네트워크에 주입하고 마지막을 제외한 모든 출력을 무시하는 것으로 댓글이 긍정적인지 부정적인지 연속적인 단어의 입력을 통해 점수를 출력하는 것이다.

 - vector-to-Sequence networ : 각 타임 스텝에서 하나의 입력 벡터를 반복해서 주입하고 하나의 시퀀스 출력하는 것이다.

그리고 인코더라고 부르는 시퀀스 투 벡터 네트워크 뒤에 디코더라고 부르는 벡터 투 시퀀스 네트워크를 연결할수도 있다.

이는 마지막 입력값이 첫번째 결과값에 영향을 미칠 수 있기 때문에 더 좋은 예측이 가능해진다.

### RNN훈련하기

RNN을 훈련할때는 타임 스텝으로 네트워크를 펼치고 역전파를 전파하는 방식인 BPTT이용한다.

여기서 오차를 역방향으로 전파할 때 마지막 출력뿐만 아니라 비용함수에 사용한 모든 출력의 각 시간 단계에서의 모델 파라미터를 조정하여 오차를 최소화한다.

### 시계열 데이터

데이터가 타임 스텝마다 하나 이상의 값을 가진 시퀀스로 타임 스텝마다 하나의 값을 가지면 단변량 시계열, 여러값을 가지면 다변량 시계열이라고 한다.

가중 이동 평균이나 자동 회귀 누적 이동 평균과 같이 시계열을 예측하는 다양한 방법이 있다.

하지만 이런 방법중에서는 트렌드나 계절성의 제거해야하는 것이 있다.

트렌드는 데이터가 시간에 따라 나타내는 장기적인 경향이나 패턴을 뜻하는 것으로 데이터가 오랜 기간 동안 증가 또는 감소하거나, 특정 방향으로 움직이는 경향을 의미한다.

계절성은 데이터에서 특정 주기로 반복되는 패턴으로 이러한 주기는 연간, 분기별, 월별, 주별 또는 일별일 수 있다.

### 긴 시퀀스 RNN

긴 시퀀스로 RNN을 훈련하면 많은 타임 스텝에 걸쳐 실행해야 함으로 RNN이 매우 깊어져 그레디언트 소실 혹은 폭주 문제가 발생 할 수 있다.

 - 불안정한 그레디언트 문제

   심층 신경망에서 사용했던 기법을 사용해서 그레디언트 문제를 완화할 수 있다.

   하지만 ReLU와 같은 수렴하지 않는 활성화 함수는 지양하는 것이 좋다.

   왜냐하면 경사 하강법이 출력을 조금 증가시키는 방향으로 가중치를 업데이트 한다고 했을 때 동일한 가중치가 모든 타임 스텝에서 사용되기 때문에 계속 출력이 증가하게 되어 폭주가 발생할 수 있기 때문이다.

   따라서 하이퍼볼릭 탄젠트 함수를 사용하면 좋다.

   그리고 그레디언트의 소실 및 폭주를 완화하기 위해 사용하였던 배치 정규화는 RNN에서 큰 성능의 향상이 없다.

   왜냐하면 배치 정규화는 훈련 배치에 대한 평균과 분산을 계산하고 입력 데이터를 정규화하는 과정을 포함하는데, RNN은 이전 시간 단계의 출력을 현재 시간 단계의 입력으로 사용하는 순차적인 구조를 가지며, 이전 시간 단계의 데이터는 현재 시간 단계의 데이터에 의존하기 때문에 배치 정규화의 사용이 어렵기 때문이다.

   그래서 RNN에서는 층 정규화 방법과 같은 다른 정규화 방법 또는 안정화 기술을 사용하는 것이 일반이다.

   층 정규화는 배치 정규화와 달리 배치 크기에 의존하지 않고, 배치 차원에 대해 정규화 하는 대신 특성 차원에 대해 정규화 한다.

   이는 샘플에 독립적으로 타임 스텝마다 동적으로 필요한 통계를 계산할 수 있어 좋다.

 - 단기 기억문제

   RNN을 거치면 데이터가 변환되므로 일부 정보는 매 훈련 스텝후 사라진다.

   따라서 어느정도 시간이 지나면 RNN에는 첫 번째 입력이 거의 남지 않게 되어 이런 문제를 해결하기 위해 장기 메모리를 가진 셀을 사용해야한다.


   - LSTM셀
  
     LSTM셀은 장단기 메모리 셀로 네트워크가 장기 상태에 저장할 것, 버릴 것, 읽어들일 것을 학습한다.

     장기 기억 네트워크에서 삭제 게이트를 지나면 일부 기억을 잃고 덧셈 연산으로 입력 게이트에서 선택한 기억을 일부 추가한다.

     그리고 이렇게 만들어진 기억은 바로 출력으로 보내지는데, 결과적으로 타임스텝마다 일부 기억이 삭제되고 일부 기억이 추가된다.

    - GRU 셀

      게이트 순환 유닛셀로 LSTM과 유사한 동작을 하지만 더 간단한 구조를 가지고 있어 계산 효율성이 높다.

      하나의 게이트 제어기 $Z_{(t)}$ 가 삭제와 입력 게이트 모두 제어하는데 삭제 게이트가 열리면 입력 게이트가 닫히는 방식으로 작동한다.

      그리고 GRU에는 따로 출력 게이트가 없는데 업데이트 게이트를 통해 전체 상태 벡터가 매 타임스텝마다 출력된다.

LSTM 계산 과정

$$
\begin{align}
& i_{(t)} = \sigma (W_{xi}^T x_{(t)} + W_{hi}^T h_{(t-1)} + b_i)\\
& f_{(t)} = \sigma (W_{xf}^T x_{(t)} + W_{hf}^T h_{(t-1)} + b_f)\\
& o_{(t)} = \sigma (W_{xo}^T x_{(t)} + W_{ho}^T h_{(t-1)} + b_o)\\
& g_{(t)} = \tanh (W_{xg}^T x_{(t)} + W_{hg}^T h_{(t-1)} + b_g)\\
& c_{(t)} = f_{(t)} \otimes c_{(t-1)} + i_{(t)} \otimes g_{(t)}\\ 
& y_{(t)} = h_{(t)} = o_{(t)} \otimes \tanh ({c}_{(t)})
\end{align}
$$

$W_{xi}, W_{xf}, W_{xo}, W_{xg}$는 입력 벡터에 연결된 네 개 층의 가중치 행렬이고, $W_{hi}, W_{hf}, W_{ho}, W_{hg}$는 이전 단기 상태에 연결된 네 개 층의 가중치 행렬이다.

b는 네 개 층 각각에 대한 편향으로 $b_f$ 를 1로 채운 벡터로 초기화 하여 훈련 초기에 모든것을 망각하는걸 방지한다.

GRU 계산 과정

$$
\begin{align}
& z_{(t)} = \sigma (W_{xz}^T x_{(t)} + W_{hz}^T h_{(t - 1)} + b_z) \\
& r_{(t)} = \sigma (W_{xr}^T x_{(t)} + W_{hr}^T h_{(t - 1)} + b_r) \\
& g_{(t)} = \tanh (W_{xg}^T x_{(t)} + W_{hg}^T (r_{(t)} \otimes h_{(t - 1)}) + b_g) \\
& h_{(t)} = (1-z_{(t)}) \otimes g_{(t)} + z_{(t)} \otimes h_{(t - 1)}
\end{align}
$$

다만 이러한 셀을 사용하더라도 매우 제한적인 단기 기억을 가져 100타임스텝 이상의 시퀀스에서 장기 패턴을 학습하는데 어려움이 있다.

이를 해결하기 위해 1차원 합성곱을 사용해 입력 시퀀스를 짧게 줄이는 방법을 사용하면 된다.

 - WAVENET

   딥러닝을 사용한 음성 생성 모델 중 하나로, 기계가 자연스러운 음성을 생성하는 데 사용되는 생성 모델이다.

   이 모델은 특별하게 층 마다 팽창 비율을 두 배로 늘리는 1차원 합성곱 층을 쌓는다.

   이렇게 하면 하위층은 단기 패턴을 학습하고 상위층은 장기 패턴을 학습아혀 아주 긴 시퀀스를 효율적으로 처리할 수 있다.

